{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n",
    "\n",
    "Here is an example of the first couple rows from the data:\n",
    "\n",
    "| id | dur       | proto | service | state | spkts | dpkts | sbytes | dbytes | rate        | sttl | dttl | sload     | dload | sloss | dloss | sinpkt | dinpkt | sjit | djit | swin | stcpb | dtcpb | dwin | tcprtt | synack | ackdat | smean | dmean | trans\\_depth | response\\_body\\_len | ct\\_srv\\_src | ct\\_state\\_ttl | ct\\_dst\\_ltm | ct\\_src\\_dport\\_ltm | ct\\_dst\\_sport\\_ltm | ct\\_dst\\_src\\_ltm | is\\_ftp\\_login | ct\\_ftp\\_cmd | ct\\_flw\\_http\\_mthd | ct\\_src\\_ltm | ct\\_srv\\_dst | is\\_sm\\_ips\\_ports | attack\\_cat | label |\n",
    "|----|-----------|-------|---------|-------|-------|-------|--------|--------|-------------|------|------|-----------|-------|-------|-------|--------|--------|------|------|------|-------|-------|------|--------|--------|--------|-------|-------|--------------|---------------------|--------------|----------------|--------------|---------------------|---------------------|-------------------|----------------|--------------|---------------------|--------------|--------------|--------------------|-------------|-------|\n",
    "| 1  | 0\\.000011 | udp   | \\-      | INT   | 2     | 0     | 496    | 0      | 90909\\.0902 | 254  | 0    | 180363632 | 0     | 0     | 0     | 0\\.011 | 0      | 0    | 0    | 0    | 0     | 0     | 0    | 0      | 0      | 0      | 248   | 0     | 0            | 0                   | 2            | 2              | 1            | 1                   | 1                   | 2                 | 0              | 0            | 0                   | 1            | 2            | 0                  | Normal      | 0     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data corresponding to exercise 1\n",
    "# Create two separate pandas dataframes for the training and test data\n",
    "# For each dataframe, import the following CSV data\n",
    "# training set: 'data/exercise1/UNSW_NB15_training-set.csv'\n",
    "# test set: 'data/exercise1/UNSW_NB15_testing-set.csv'\n",
    "\n",
    "df_train = pd.read_csv('data/exercise1/UNSW_NB15_training-set.csv')\n",
    "df_test = pd.read_csv('data/exercise1/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 \n",
    "Keep the two dataframes separate and create train/test data and labels.  This will be used to experiment with the case where there are different types of activities in the training versus test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all the following operations in the cell on both the dataframes separately\n",
    "# 1) Keep only the datapoints where the 'attack_cat' column is equal to either 'Normal' or 'Fuzzers'\n",
    "# CODE HERE\n",
    "df_train = df_train[df_train['attack_cat'].isin(['Normal', 'Fuzzers'])]\n",
    "df_test = df_test[df_test['attack_cat'].isin(['Normal', 'Fuzzers'])]\n",
    "\n",
    "# 2) Get the labels from the dataframe (i.e., the values in the 'attack_cat' column)\n",
    "# CODE HERE\n",
    "labels_train = df_train['attack_cat'].values\n",
    "labels_test = df_test['attack_cat'].values\n",
    "\n",
    "# 3) Keep only the features we care about for this experiment.\n",
    "# We only care about the numerical features between column 'spkts' and 'is_sm_ips_ports' (inclusive)\n",
    "# CODE HERE\n",
    "features_train = df_train.iloc[:, df_train.columns.get_loc('spkts'):df_train.columns.get_loc('is_sm_ips_ports')+1].values\n",
    "features_test = df_test.iloc[:, df_test.columns.get_loc('spkts'):df_test.columns.get_loc('is_sm_ips_ports')+1].values\n",
    "\n",
    "# You should now have four inputs usable for scikit-learn:\n",
    "# training data\n",
    "# training labels\n",
    "# test data\n",
    "# test labels\n",
    "# Hint: You may have to add some minor code in the above to get the data ready for scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on training data:\n",
      "Precision:  0.9998606632343643\n",
      "Recall:  0.9998606660164414\n",
      "F1:  0.9998606467896146\n",
      "Accuracy:  0.9998606660164414\n",
      "\n",
      "Metrics on test data:\n",
      "Precision:  0.8158847314454788\n",
      "Recall:  0.7986493044322226\n",
      "F1:  0.7470410366701024\n",
      "Accuracy:  0.7986493044322226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_train = RandomForestClassifier()\n",
    "rfc_train.fit(features_train, labels_train)\n",
    "predict = rfc_train.predict(features_train)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print('Metrics on training data:')\n",
    "print('Precision: ', precision_score(labels_train, predict, average='weighted'))\n",
    "print('Recall: ', recall_score(labels_train, predict, average='weighted'))\n",
    "print('F1: ', f1_score(labels_train, predict, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(labels_train, predict))\n",
    "\n",
    "predict = rfc_train.predict(features_test)\n",
    "\n",
    "print('\\nMetrics on test data:')\n",
    "print('Precision: ', precision_score(labels_test, predict, average='weighted'))\n",
    "print('Recall: ', recall_score(labels_test, predict, average='weighted'))\n",
    "print('F1: ', f1_score(labels_test, predict, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(labels_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 \n",
    "Create a new training/test split by combining the dataframes into one.  Then split the dataframe randomly into train/test data and labels.  This will be used to experiment with the case where there are largely the same types of activities in the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes into a single dataframe, then do the following\n",
    "# CODE HERE\n",
    "df = pd.concat([df_train, df_test])\n",
    "\n",
    "# 1) Keep only the datapoints where the 'attack_cat' column is equal to either 'Normal' or 'Fuzzers'\n",
    "# CODE HERE\n",
    "df = df[df['attack_cat'].isin(['Normal', 'Fuzzers'])]\n",
    "\n",
    "# 2) Get the labels from the dataframe (i.e., the values in the 'attack_cat' column)\n",
    "# CODE HERE\n",
    "labels = df['attack_cat'].values\n",
    "\n",
    "# 3) Keep only features we care about for this experiment.\n",
    "# We only care about the numerical features between column 'spkts' and 'is_sm_ips_ports' (inclusive)\n",
    "# CODE HERE\n",
    "features = df.iloc[:, df.columns.get_loc('spkts'):df.columns.get_loc('is_sm_ips_ports')+1].values\n",
    "\n",
    "# 4) Create a random split; put 50% of the data into the training set and the other 50% into the test set\n",
    "# Use scikit-learn's 'train_test_split'\n",
    "# Hint: You may have to add some minor code in the above to get the data ready for scikit-learn's 'train_test_split'\n",
    "# CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.5)\n",
    "\n",
    "# You should now have four inputs usable for scikit-learn that are distinct from the inputs you created in Part 1.\n",
    "# The inputs should correspond to:\n",
    "# training data\n",
    "# training labels\n",
    "# test data\n",
    "# test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each of the train/test splits, create a separate random forest with default sklearn parameters\n",
    "# Hint: Ignoring import statements, each random forest can be created in a single line of code\n",
    "# CODE HERE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_train = RandomForestClassifier()\n",
    "rfc_train.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on training data:\n",
      "Precision:  0.9956310058043459\n",
      "Recall:  0.9956331132831824\n",
      "F1:  0.9956319171995252\n",
      "Accuracy:  0.9956331132831824\n",
      "\n",
      "Metrics on test data:\n",
      "Precision:  0.8995047506717183\n",
      "Recall:  0.9016938744178906\n",
      "F1:  0.90032090702092\n",
      "Accuracy:  0.9016938744178906\n"
     ]
    }
   ],
   "source": [
    "# For each of the train/test splits and associated random forest, do the following:\n",
    "\n",
    "# 1) Predict labels on the training data\n",
    "# CODE HERE\n",
    "predict = rfc_train.predict(features_train)\n",
    "\n",
    "# 2) Print metrics on the training data; use sklearn's implementation of precision, recall, f1, and accuracy\n",
    "# CODE HERE\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print('Metrics on training data:')\n",
    "print('Precision: ', precision_score(labels_train, predict, average='weighted'))\n",
    "print('Recall: ', recall_score(labels_train, predict, average='weighted'))\n",
    "print('F1: ', f1_score(labels_train, predict, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(labels_train, predict))\n",
    "\n",
    "# 3) Predict labels on the test data\n",
    "# CODE HERE\n",
    "predict = rfc_train.predict(features_test)\n",
    "\n",
    "# 4) Print metrics on the test data; again, use precision, recall, f1, and accuracy\n",
    "# CODE HERE\n",
    "print('\\nMetrics on test data:')\n",
    "print('Precision: ', precision_score(labels_test, predict, average='weighted'))\n",
    "print('Recall: ', recall_score(labels_test, predict, average='weighted'))\n",
    "print('F1: ', f1_score(labels_test, predict, average='weighted'))\n",
    "print('Accuracy: ', accuracy_score(labels_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1) For results using Part 1 data, what is the precision and recall?\n",
    "\n",
    "Precision: 0.82\n",
    "Recall: 0.80\n",
    "\n",
    "2) For results using Part 1 data, describe the difference in the results on the training and test data. What does this signify? \n",
    "\n",
    "The training data has a higher precision and recall than the test data. This signifies that the model is overfits the training data, while not generalizing well to the test data.\n",
    "\n",
    "3) What changes in the results on the test data once you combine the data for Part 2? Does this produce a better classifier? Why or why not?\n",
    "\n",
    "Precision: 0.90\n",
    "Recall: 0.90\n",
    "\n",
    "The results on the test data are better once the data is combined for Part 2. This produces a better classifier because the model is trained on a more diverse set of data, and is able to generalize better to the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2022.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
