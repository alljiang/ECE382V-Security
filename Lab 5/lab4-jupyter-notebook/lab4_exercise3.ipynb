{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n",
    "\n",
    "The data is separated into three folders: Attack_Data_Master, Training_Data_Master, and Validation_Data_Master\n",
    "These can be found here:\n",
    "data/exercise3/Training_Data_Master\n",
    "data/exercise3/Validation_Data_Master\n",
    "data/exercise3/Attack_Data_Master\n",
    "\n",
    "All of the data in Training_Data_Master and Validation_Data_Master is normal, \n",
    "and all the data in Attack_Data_Master is malicious\n",
    "\n",
    "For the purpose of this exercise, you will ignore the predefined training/validation splits, and simply use Training_Data_Master\n",
    "and Validation_Data_Master as a single pool of normal data\n",
    "\n",
    "As mentioned, each system call trace is stored as a single file.  Treat each system call trace as a separate datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the normal system call traces (i.e., everything in Training_Data_Master and Validation_Data_Master)\n",
    "\n",
    "# CODE HERE\n",
    "data_benign = []\n",
    "data_all = []\n",
    "data_malicious = []\n",
    "is_malicious = []\n",
    "path_training = 'data/exercise3/Training_Data_Master/'\n",
    "for filename in os.listdir(path_training):\n",
    "    with open(path_training + filename, 'r') as f:\n",
    "        data = f.read()\n",
    "        # add a 0 in front of each single digit number\n",
    "        for i in range(10):\n",
    "            data = data.replace(' ' + str(i) + ' ', ' 0' + str(i) + ' ')\n",
    "            data = data.replace(' ' + str(i) + ' ', ' 0' + str(i) + ' ')\n",
    "\n",
    "        # if first or last numbers are single digit, add a 0 in front\n",
    "        if data[1] == ' ':\n",
    "            data = '0' + data\n",
    "        if data[-2] == ' ':\n",
    "            data = data[:-2] + ' 0' + data[-1:]\n",
    "        \n",
    "        data_all.append(data)\n",
    "        data_benign.append(data)\n",
    "        is_malicious.append(0)\n",
    "path_validation = 'data/exercise3/Validation_Data_Master/'\n",
    "for filename in os.listdir(path_validation):\n",
    "    with open(path_validation + filename, 'r') as f:\n",
    "        data = f.read()\n",
    "        # add a 0 in front of each single digit number\n",
    "        for i in range(10):\n",
    "            data = data.replace(' ' + str(i) + ' ', ' 0' + str(i) + ' ')\n",
    "\n",
    "        # if first or last numbers are single digit, add a 0 in front\n",
    "        if data[1] == ' ':\n",
    "            data = '0' + data\n",
    "        if data[-2] == ' ':\n",
    "            data = data[:-2] + ' 0' + data[-1:]\n",
    "\n",
    "        data_all.append(data)\n",
    "        data_benign.append(data)\n",
    "        is_malicious.append(0)\n",
    "\n",
    "# Load all the malicious system call traces (i.e., everything in Attack_Data_Master)\n",
    "# CODE HERE\n",
    "path_malicious = 'data/exercise3/Attack_Data_Master/'\n",
    "for folder in os.listdir(path_malicious):\n",
    "    folder_path = path_malicious + folder + '/'\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with open(folder_path + filename, 'r') as f:\n",
    "            data = f.read()\n",
    "            # add a 0 in front of each single digit number\n",
    "            for i in range(10):\n",
    "                data = data.replace(' ' + str(i) + ' ', ' 0' + str(i) + ' ')\n",
    "\n",
    "            # if first or last numbers are single digit, add a 0 in front\n",
    "            if data[1] == ' ':\n",
    "                data = '0' + data\n",
    "            if data[-2] == ' ':\n",
    "                data = data[:-2] + ' 0' + data[-1:]\n",
    "            \n",
    "            data_all.append(data)\n",
    "            data_malicious.append(data)\n",
    "            is_malicious.append(1)\n",
    "\n",
    "# Hint: A useful way to load this is as one or two Python lists, where each entry in the list corresponds to the text string\n",
    "#       of system calls ids; feel free to use a single list for all the data, or separate lists for malicious versus normal\n",
    "#       data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Tokenize and create a dataset where each datapoint corresponds to (normalized) counts of \n",
    "system call n-grams. Try various sizes of ngrams.\n",
    "\n",
    "Reminder: A sequence of system call IDs that looks like this:\n",
    "'6 6 63 6 42'\n",
    "\n",
    "contains the following 3-grams:\n",
    "'6 6 63'\n",
    "'6 63 6'\n",
    "'63 6 42'\n",
    "\n",
    "Note: There are a number of ways you could code this up, but if you loaded the data\n",
    "as lists of strings, you could consider using some of the feature extraction methods in \n",
    "sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Allen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Look at the classdemo notebook for an example of doing this\n",
    "# CODE HERE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(3,3))  # character n-gram feature extraction\n",
    "\n",
    "# Extract feature counts\n",
    "raw_counts = count_vect.fit_transform(data_all)\n",
    "\n",
    "features = count_vect.get_feature_names()\n",
    "\n",
    "# print(raw_counts)\n",
    "# print(raw_counts.toarray())\n",
    "# print('Feature set: ' + str(features))\n",
    "# print('Number of features: ' + str(len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 50% of the data for the training set and the rest for the test set\n",
    "# CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_counts, is_malicious, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use Logistic Regression for this exercise\n",
    "# Feel free to experiment with the various hyperparameters available to you in sklearn\n",
    "# CODE HERE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=100000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7857142857142857\n",
      "Recall: 0.8725212464589235\n",
      "F1-measure: 0.8268456375838926\n",
      "Accuracy: 0.9566532258064516\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the test data and predict labels for each data point in the test data\n",
    "# CODE HERE\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the following metrics: precision, recall, f1-measure, and accuracy\n",
    "# CODE HERE\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print('Precision: ' + str(precision_score(y_test, y_pred)))\n",
    "print('Recall: ' + str(recall_score(y_test, y_pred)))\n",
    "print('F1-measure: ' + str(f1_score(y_test, y_pred)))\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram: 1\n",
      "Precision: 0.7204030226700252\n",
      "Recall: 0.8101983002832861\n",
      "F1-measure: 0.7626666666666666\n",
      "Accuracy: 0.9401881720430108\n",
      "\n",
      "N-gram: 2\n",
      "Precision: 0.8010899182561307\n",
      "Recall: 0.8328611898016998\n",
      "F1-measure: 0.8166666666666667\n",
      "Accuracy: 0.9556451612903226\n",
      "\n",
      "N-gram: 3\n",
      "Precision: 0.7857142857142857\n",
      "Recall: 0.8725212464589235\n",
      "F1-measure: 0.8268456375838926\n",
      "Accuracy: 0.9566532258064516\n",
      "\n",
      "N-gram: 4\n",
      "Precision: 0.7663316582914573\n",
      "Recall: 0.8640226628895185\n",
      "F1-measure: 0.8122503328894808\n",
      "Accuracy: 0.9526209677419355\n",
      "\n",
      "N-gram: 5\n",
      "Precision: 0.780281690140845\n",
      "Recall: 0.7847025495750708\n",
      "F1-measure: 0.7824858757062146\n",
      "Accuracy: 0.948252688172043\n",
      "\n",
      "N-gram: 6\n",
      "Precision: 0.8421052631578947\n",
      "Recall: 0.7252124645892352\n",
      "F1-measure: 0.7792998477929985\n",
      "Accuracy: 0.9512768817204301\n",
      "\n",
      "N-gram: 7\n",
      "Precision: 0.8467153284671532\n",
      "Recall: 0.6572237960339944\n",
      "F1-measure: 0.7400318979266348\n",
      "Accuracy: 0.9452284946236559\n",
      "\n",
      "N-gram: 8\n",
      "Precision: 0.8484848484848485\n",
      "Recall: 0.6345609065155807\n",
      "F1-measure: 0.7260940032414911\n",
      "Accuracy: 0.9432123655913979\n",
      "\n",
      "N-gram: 9\n",
      "Precision: 0.8647540983606558\n",
      "Recall: 0.5977337110481586\n",
      "F1-measure: 0.7068676716917922\n",
      "Accuracy: 0.9411962365591398\n",
      "\n",
      "N-gram: 10\n",
      "Precision: 0.8546255506607929\n",
      "Recall: 0.5495750708215298\n",
      "F1-measure: 0.6689655172413793\n",
      "Accuracy: 0.9354838709677419\n",
      "\n",
      "N-gram: 11\n",
      "Precision: 0.8826530612244898\n",
      "Recall: 0.49008498583569404\n",
      "F1-measure: 0.6302367941712205\n",
      "Accuracy: 0.9317876344086021\n",
      "\n",
      "N-gram: 12\n",
      "Precision: 0.888235294117647\n",
      "Recall: 0.42776203966005666\n",
      "F1-measure: 0.5774378585086042\n",
      "Accuracy: 0.925739247311828\n",
      "\n",
      "N-gram: 13\n",
      "Precision: 0.8896551724137931\n",
      "Recall: 0.3654390934844193\n",
      "F1-measure: 0.5180722891566265\n",
      "Accuracy: 0.9193548387096774\n",
      "\n",
      "N-gram: 14\n",
      "Precision: 0.8778625954198473\n",
      "Recall: 0.32577903682719545\n",
      "F1-measure: 0.47520661157024785\n",
      "Accuracy: 0.9146505376344086\n",
      "\n",
      "N-gram: 15\n",
      "Precision: 0.8677685950413223\n",
      "Recall: 0.29745042492917845\n",
      "F1-measure: 0.4430379746835442\n",
      "Accuracy: 0.9112903225806451\n",
      "\n",
      "N-gram: 16\n",
      "Precision: 0.8706896551724138\n",
      "Recall: 0.28611898016997167\n",
      "F1-measure: 0.4307036247334755\n",
      "Accuracy: 0.9102822580645161\n",
      "\n",
      "N-gram: 17\n",
      "Precision: 0.8761061946902655\n",
      "Recall: 0.2804532577903683\n",
      "F1-measure: 0.424892703862661\n",
      "Accuracy: 0.9099462365591398\n",
      "\n",
      "N-gram: 18\n",
      "Precision: 0.8785046728971962\n",
      "Recall: 0.26628895184135976\n",
      "F1-measure: 0.408695652173913\n",
      "Accuracy: 0.9086021505376344\n",
      "\n",
      "N-gram: 19\n",
      "Precision: 0.883495145631068\n",
      "Recall: 0.2577903682719547\n",
      "F1-measure: 0.3991228070175439\n",
      "Accuracy: 0.9079301075268817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):\n",
    "    count_vect = CountVectorizer(analyzer='word', ngram_range=(i,i))  # character n-gram feature extraction\n",
    "    raw_counts = count_vect.fit_transform(data_all)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(raw_counts, is_malicious, test_size=0.5, random_state=0)\n",
    "    clf = LogisticRegression(max_iter=100000).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print('N-gram: ' + str(i))\n",
    "    print('Precision: ' + str(precision_score(y_test, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_test, y_pred)))\n",
    "    print('F1-measure: ' + str(f1_score(y_test, y_pred)))\n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Varying class priors\n",
    "\n",
    "Create several new test datasets where you have randomly subsampled the number of \n",
    "attack datapoints.\n",
    "\n",
    "In particular, create the following datasets:\n",
    "- 10 datasets where 25% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 50% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 75% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 90% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 95% of the attack datapoints are removed from the original test set\n",
    "\n",
    "Report five sets of precision, recall, f1-measure, and accuracy corresponding to the following:\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 25% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 50% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 75% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 90% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 95% of attack datapoints removed\n",
    "\n",
    "Note: You will use the same model trained in part 1 for all of these datasets.  \n",
    "All you are varying is the class priors during the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets of the test set by randomly discarding X% of points with label +1\n",
    "# CODE HERE\n",
    "import random\n",
    "\n",
    "datasets_25 = []\n",
    "datasets_25_y = []\n",
    "for i in range(10):\n",
    "    new_malicious_data = random.sample(data_malicious, int(len(data_malicious) * 0.25))\n",
    "    new_set = []\n",
    "    new_set = np.concatenate((data_benign, new_malicious_data))\n",
    "    \n",
    "    if i == 0:\n",
    "        datasets_25_y = np.concatenate(([0] * len(data_benign), [1] * len(new_malicious_data)))\n",
    "\n",
    "    datasets_25.append(new_set)   \n",
    "\n",
    "datasets_50 = []\n",
    "datasets_50_y = []\n",
    "for i in range(10):\n",
    "    new_malicious_data = random.sample(data_malicious, int(len(data_malicious) * 0.50))\n",
    "    new_set = []\n",
    "    new_set = np.concatenate((data_benign, new_malicious_data))\n",
    "    \n",
    "    if i == 0:\n",
    "        datasets_50_y = np.concatenate(([0] * len(data_benign), [1] * len(new_malicious_data)))\n",
    "\n",
    "    datasets_50.append(new_set)\n",
    "\n",
    "datasets_75 = []\n",
    "datasets_75_y = []\n",
    "for i in range(10):\n",
    "    new_malicious_data = random.sample(data_malicious, int(len(data_malicious) * 0.75))\n",
    "    new_set = []\n",
    "    new_set = np.concatenate((data_benign, new_malicious_data))\n",
    "    \n",
    "    if i == 0:\n",
    "        datasets_75_y = np.concatenate(([0] * len(data_benign), [1] * len(new_malicious_data)))\n",
    "\n",
    "    datasets_75.append(new_set)\n",
    "\n",
    "datasets_90 = []\n",
    "datasets_90_y = []\n",
    "for i in range(10):\n",
    "    new_malicious_data = random.sample(data_malicious, int(len(data_malicious) * 0.90))\n",
    "    new_set = []\n",
    "    new_set = np.concatenate((data_benign, new_malicious_data))\n",
    "    \n",
    "    if i == 0:\n",
    "        datasets_90_y = np.concatenate(([0] * len(data_benign), [1] * len(new_malicious_data)))\n",
    "\n",
    "    datasets_90.append(new_set)\n",
    "\n",
    "datasets_95 = []\n",
    "datasets_95_y = []\n",
    "for i in range(10):\n",
    "    new_malicious_data = random.sample(data_malicious, int(len(data_malicious) * 0.95))\n",
    "    new_set = []\n",
    "    new_set = np.concatenate((data_benign, new_malicious_data))\n",
    "    \n",
    "    if i == 0:\n",
    "        datasets_95_y = np.concatenate(([0] * len(data_benign), [1] * len(new_malicious_data)))\n",
    "\n",
    "    datasets_95.append(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25%\n",
      "Precision: 0.6501269603107982\n",
      "Recall: 0.6210526315789474\n",
      "F1-measure: 0.6344260188813885\n",
      "Accuracy: 0.9748516320474778\n",
      "50%\n",
      "Precision: 0.7358059349461118\n",
      "Recall: 0.7522471910112359\n",
      "F1-measure: 0.7434868265590182\n",
      "Accuracy: 0.9668698458228755\n",
      "75%\n",
      "Precision: 0.7583795825859747\n",
      "Recall: 0.8333333333333334\n",
      "F1-measure: 0.7939675055844712\n",
      "Accuracy: 0.9599236641221374\n",
      "90%\n",
      "Precision: 0.7941176265900957\n",
      "Recall: 0.8379939209726445\n",
      "F1-measure: 0.8153137897773742\n",
      "Accuracy: 0.9574880871341047\n",
      "95%\n",
      "Precision: 0.8035611860788896\n",
      "Recall: 0.8463343108504399\n",
      "F1-measure: 0.8242989025372921\n",
      "Accuracy: 0.9584037876225906\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(x, y):\n",
    "    raw_counts = count_vect.fit_transform(x)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(raw_counts, y, test_size=0.5, random_state=0)\n",
    "    clf = LogisticRegression(max_iter=100000).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    return [precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred), accuracy_score(y_test, y_pred)]\n",
    "\n",
    "# 25%\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    metrics = get_metrics(datasets_25[i], datasets_25_y)\n",
    "    precisions.append(metrics[0])\n",
    "    recalls.append(metrics[1])\n",
    "    f1s.append(metrics[2])\n",
    "    accuracies.append(metrics[3])\n",
    "\n",
    "print('25%')\n",
    "print('Precision: ' + str(np.mean(precisions)))\n",
    "print('Recall: ' + str(np.mean(recalls)))\n",
    "print('F1-measure: ' + str(np.mean(f1s)))\n",
    "print('Accuracy: ' + str(np.mean(accuracies)))\n",
    "\n",
    "# 50%\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    metrics = get_metrics(datasets_50[i], datasets_50_y)\n",
    "    precisions.append(metrics[0])\n",
    "    recalls.append(metrics[1])\n",
    "    f1s.append(metrics[2])\n",
    "    accuracies.append(metrics[3])\n",
    "\n",
    "print('50%')\n",
    "print('Precision: ' + str(np.mean(precisions)))\n",
    "print('Recall: ' + str(np.mean(recalls)))\n",
    "print('F1-measure: ' + str(np.mean(f1s)))\n",
    "print('Accuracy: ' + str(np.mean(accuracies)))\n",
    "\n",
    "# 75%\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    metrics = get_metrics(datasets_75[i], datasets_75_y)\n",
    "    precisions.append(metrics[0])\n",
    "    recalls.append(metrics[1])\n",
    "    f1s.append(metrics[2])\n",
    "    accuracies.append(metrics[3])\n",
    "\n",
    "print('75%')\n",
    "print('Precision: ' + str(np.mean(precisions)))\n",
    "print('Recall: ' + str(np.mean(recalls)))\n",
    "print('F1-measure: ' + str(np.mean(f1s)))\n",
    "print('Accuracy: ' + str(np.mean(accuracies)))\n",
    "\n",
    "# 90%\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    metrics = get_metrics(datasets_90[i], datasets_90_y)\n",
    "    precisions.append(metrics[0])\n",
    "    recalls.append(metrics[1])\n",
    "    f1s.append(metrics[2])\n",
    "    accuracies.append(metrics[3])\n",
    "\n",
    "print('90%')\n",
    "print('Precision: ' + str(np.mean(precisions)))\n",
    "print('Recall: ' + str(np.mean(recalls)))\n",
    "print('F1-measure: ' + str(np.mean(f1s)))\n",
    "print('Accuracy: ' + str(np.mean(accuracies)))\n",
    "\n",
    "# 95%\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    metrics = get_metrics(datasets_95[i], datasets_95_y)\n",
    "    precisions.append(metrics[0])\n",
    "    recalls.append(metrics[1])\n",
    "    f1s.append(metrics[2])\n",
    "    accuracies.append(metrics[3])\n",
    "\n",
    "print('95%')\n",
    "print('Precision: ' + str(np.mean(precisions)))\n",
    "print('Recall: ' + str(np.mean(recalls)))\n",
    "print('F1-measure: ' + str(np.mean(f1s)))\n",
    "print('Accuracy: ' + str(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1) In Part 1, what size of ngrams gives the best performance? What are the tradeoffs as you change the size?\n",
    "\n",
    "n=3 gives the highest f1 score.\n",
    "As n increases, the precision increases, but the recall decreases, due to the loss of generality. The accuracy also decreases.\n",
    "\n",
    "2) In Part 1, how does performance change if we use simple counts as features (i.e., 1-grams) as opposed to counts of 2-grams? What does this tell you about the role of sequences in prediction for this dataset?\n",
    "\n",
    "There is a lower score in all aspects when using 1-grams. This tells us that the sequences are important to the predictions for this dataset. We would have trouble predicting the correct class if we only looked at 1 system call at a time.\n",
    "\n",
    "3) How does performance change as a function of class prior in Part 2?\n",
    "\n",
    "As the class prior increases, the F1 scores increases. The precision and recall also increases, while the accuracy decreases. As more attack data is removed, the model is more likely to predict the normal class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
